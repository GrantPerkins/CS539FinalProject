{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "Grant Perkins\n",
    "\n",
    "## Problem Statement\n",
    "Can features be derived from the MBTA's reliability dataset to classify whether the Red Line Rapid Transit will be late during peak hours to at least 75% accuracy?\n",
    "\n",
    "### Background\n",
    "The Massachusetts Bay Transportation (MBTA) is the primary operator of public transportation in the greater Boston area. They provide consistent commuter rails, rapid transit, and buses for a minimal fee. The MBTA has a commitment to providing “accurate and timely information” about delays in their\n",
    "services (https://www.mbta.com/policies/our-commitment-you). Predicting if the MBTA’s Red Line Rapid Transit will be late, based on the past month of data about the historical number of people trying to ride MBTA Rapid Transit and how many people got on the trains on time, would improve accuracy of reported information, in line with business goals. I will initially attempt using a Recurrent Neural Network (RNN) to solve this problem.\n",
    "\n",
    "### Dataset Description\n",
    "The dataset can be found here: [https://mbta-massdot.opendata.arcgis.com/datasets/MassDOT::mbta-bus-commuter-rail-rapid-transit-reliability/about](https://mbta-massdot.opendata.arcgis.com/datasets/MassDOT::mbta-bus-commuter-rail-rapid-transit-reliability/about)\n",
    "\n",
    "The dataset I will use is the “MBTA Bus, Commuter Rail, & Rapid Transit Reliability”. This dataset is a daily report of reliability for all bus, commuter rails, and rapid transit options through the MBTA. It provides a timestamp, mode of transportation, number of people who used transport, and number of people who wanted to use transport. There are 8 million rows in this dataset, with timestamps ranging back from 2015 to the start of December in 2022. There is exactly one row for each day. It is not intraday data unfortunately, so there is no data about what specific times trains were late or canceled.\n",
    "\n",
    "This dataset is easily accessible as a CSV file with over 8 million rows. The dataset is updated every few months, as the last update was November 30, 2022. I will only be using this data for this project. I will be removing a large chunk of this dataset, as I am only interested in Rapid Transit trains during peak hours.\n",
    "\n",
    "# EDA Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "In this section, I am loading a preprocessing the dataset for training and prediction. The following steps are taken:\n",
    " - Load raw csv\n",
    " - Isolate rapid transit trains during peak hours\n",
    " - Reshape df from one row for each day, each line, to one row for each day with columns for each line\n",
    " - Interpolate missing values with medians per column (from EDA)\n",
    " - Save df for future use\n",
    " - Separate prediction variables from labels (red line reliability)\n",
    " - Use sliding window to create 30 day periods to predict\n",
    " - Split dataset 60:20:20 train:validation:test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load raw csv\n",
    "df = pd.read_csv(r\"C:\\Users\\gcper\\OneDrive\\WPI\\ML\\final\\MBTA_Reliability.csv\")\n",
    "\n",
    "# isolate rapid transit trains during peak hours\n",
    "df = df[(df[\"gtfs_route_desc\"] == \"Rapid Transit\") & (df[\"mode_type\"] == \"Rail\") & (df[\"peak_offpeak_ind\"] == \"PEAK\")]\n",
    "df[\"service_date\"] = df[\"service_date\"].apply(lambda s: s.split()[0])\n",
    "routes = df[\"gtfs_route_id\"].unique()\n",
    "\n",
    "# reshape df\n",
    "# make template df, one row for each date\n",
    "new_df = pd.DataFrame({\"service_date\": df.service_date.unique()})\n",
    "for route in routes:\n",
    "    # make a column for each route's numerator and denominator\n",
    "    route_df = df[df.gtfs_route_id == route][[\"service_date\", \"otp_numerator\", \"otp_denominator\"]]\n",
    "    route_df = route_df.rename({\"otp_numerator\": f\"{route}_Numerator\", \"otp_denominator\": f\"{route}_Denominator\"},\n",
    "                               errors=\"raise\", axis=1)\n",
    "    new_df = new_df.merge(route_df, left_on=\"service_date\", right_on=\"service_date\",\n",
    "                          how=\"left\")  # puts NaNs where data is missing (how=left)\n",
    "new_df.service_date = pd.to_datetime(new_df.service_date, format=\"%Y/%m/%d\",\n",
    "                                     errors=\"raise\")  # convert service date to date type\n",
    "new_df = new_df.sort_values(by=\"service_date\", ascending=True)  # sort df by date, oldest to newest\n",
    "new_df = new_df.rename({\"service_date\": \"Date\"}, axis=1)\n",
    "new_df = new_df.set_index(\"Date\")\n",
    "\n",
    "# interpolate\n",
    "for col in new_df.columns:\n",
    "    new_df[col].fillna(value=new_df[col].median(), inplace=True)\n",
    "\n",
    "# save final df\n",
    "new_df.to_csv(\"mbta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(mbta_df, window=30, val_split=0.2, test_split=0.2):\n",
    "    # separate input and labels\n",
    "    red_line = mbta_df[\"Red_Numerator\"] / mbta_df[\"Red_Denominator\"]\n",
    "    variables_df = mbta_df[[col for col in mbta_df.columns if \"Red\" not in col]]\n",
    "    \n",
    "    # sliding window\n",
    "    windows = variables_df.rolling(window=window)\n",
    "    X = [window.to_numpy() for window in windows]\n",
    "    X = X[window-1:-1] # first `window` windows are cut short, drop last window as well (can't predict)\n",
    "    X = np.array(X)\n",
    "    y = red_line.to_numpy()\n",
    "    y = 1.0 * (y < 0.9) # define unreliable as < 90%\n",
    "    y = y[window:] # offset labels. trying to predict the NEXT day\n",
    "    \n",
    "    # train - validation - test split\n",
    "    # 60 : 20 : 20\n",
    "    length = len(y)\n",
    "    train_length = int((1 - val_split - test_split) * length)\n",
    "    val_length = int(val_split * length)\n",
    "    test_length = int(test_split * length)\n",
    "    train_X, train_y = X[:train_length], y[:train_length]\n",
    "    val_X, val_y = X[train_length:train_length+val_length], y[train_length:train_length+val_length]\n",
    "    test_X, test_y = X[train_length+val_length:], y[train_length+val_length:]\n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1062, 30, 13), (1062,), (354, 30, 13), (354,), (355, 30, 13), (355,)]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mbta.csv\")\n",
    "train_X, train_y, val_X, val_y, test_X, test_y = load_dataset(df)\n",
    "print([i.shape for i in [train_X, train_y, val_X, val_y, test_X, test_y]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
